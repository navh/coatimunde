{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww14080\viewh18000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \\documentclass\{article\}\
\\usepackage[utf8]\{inputenc\}\
\\usepackage\{fancyhdr\}\
\\usepackage[margin=2.9cm,twoside]\{geometry\}\
\\usepackage[super]\{nth\}\
\\usepackage[english]\{babel\}\
\\usepackage\{csquotes\}\
\
\\usepackage\{hyperref\}\
\\usepackage[backend=biber,style=ieee]\{biblatex\}\
\\addbibresource\{bibliographyDID03.bib\}\
\
\\pagestyle\{fancy\}\
\\fancyhf\{\}\
\\fancyhead[LE,RO]\{DID - 03 SOR (Hebb \\& Stephan)\}\
\\fancyhead[LO,RE]\{\\leftmark\}\
\\fancyfoot[LE,RO]\{\\thepage\}\
\
% Title Page\
\\title\{Statement of Requirements\}\
\\author\{Officer Cadet 27714 Amos Navarre Hebb\\\\and\\\\Officer Cadet 27555 Kara Stephan\}\
\
\\usepackage\{graphicx\}\
\
\\begin\{document\}\
	\
\\begin\{titlepage\}\
	\\begin\{center\}\
		\\vspace*\{1cm\}\
		\
		\\LARGE\\textsc\{Royal Military College of Canada\}\\normalsize\
		\
		\\vspace\{0.2cm\}\
		\
		\\textsc\{Department of Electrical and Computer Engineering\}\
		\
		\\vspace\{1.5cm\}\
		\
		\\includegraphics[width=0.3\\textwidth]\{rmcLogo.png\}\
		\
		\\vspace\{1.5cm\}\
		\
		\\LARGE\{Project Coatimunde Requirements\\\\\}\
		\
		\\vspace\{0.2cm\}\
		\
		\\normalsize\{Computer Optics Analyzing Trajectories In Mostly Unknown, Navigation Denied, Environments\}\
		\
		\\vspace\{0.1cm\}\
		\
		\\normalsize\{DID-04 - Preliminary Design\}\
		\
		\\vfill\
		\
		\\textbf\{Presented by:\}\\\\Amos Navarre \\textsc\{Hebb\} \\& Kara \\textsc\{Stephan\}\\\\\
		\\vspace\{0.8cm\}\
		\\textbf\{Presented to:\}\\\\Dr. Sidney \\textsc\{Givigi\} \\& Rachid \\textsc\{Beguenane\}\
		\\vspace\{0.8cm\}\
		\
		\\today\
		\
	\\end\{center\}\
\\end\{titlepage\}\
\
% \\begin\{abstract\}\
% \\end\{abstract\}\
\
\\tableofcontents\
\\newpage\
\
\\section\{Introduction\}\
\
	\\subsection\{Document Purpose\}\
	\
	Using Computer Optics for Analyzing Trajectories in Mostly Unknown, Navigation Denied, Environments (\\textsc\{coatimunde\}) is the goal of this project. The purpose of this document is to outline the requirements for \\textsc\{coatimunde\}. That is, what the project is to accomplish once all of the requirements outlined in this document have been completed and to what standard they shall be considered done. The benefits of meeting these requirements and solving this problem will be outlined and some possible use cases stated. This document will then identify the constraints that these requirements impose on this project.\
	\
	\\subsection\{Background\}\
	\
	Both in the consumer and professional sectors the use of autonomous aerial vehicles is growing quickly. Currently these vehicles rely on skilled pilots to accomplish a very limited set of tasks. Adding obstacle avoidance capabilities to these vehicles and simplifying the task of following targets could allow for these systems to be used in many more situations. This section will give a quick background on obstacle avoidance, unmanned aircraft systems, computer vision, and the platforms we intend to use in this project.\
	\
		\\subsubsection\{Obstacle Avoidance\}\
		\
		\
		Obstacle avoidance is the task of satisfying a control objective, in this case moving toward a visual target, while subject to non-intersection or non-collision position constraints. The latter constraints are, in this case, to be dynamically created while moving in a reactive manner, instead of being pre-computed.\
		\
		\\subsubsection\{Unmanned Aircraft Systems\}\
		\
		Very generally any powered vehicle that uses aerodynamic forces to provide lift without a human operator being carried can be considered an unmanned aerial vehicle. Currently most of these vehicles make up a single component of a larger unmanned aircraft system. \
		\
		An Unmanned aircraft system (UAS), or remotely piloted aircraft system (RPAS), is an aircraft without a human pilot on-board, instead controlled from an operator on the ground. Such a system can have varying levels of autonomy, something as simple as a model aircraft could be considered a UAS without any automation capabilities. Detecting, recognizing, identifying, and tracking targets of interest in complex environments and integrate with the systems required to process and fuse the collected information into actionable intelligence while operating in a low-to-medium threat environment is the current goal of the RPAS project by the Royal Canadian Air Force (RCAF) \\cite\{RPAS\}. \
		\
		Flying a UAS requires a secure link to the operator off-board. Maintaining this link, particularly while flying close to the ground where more opportunities for interference are introduced is difficult. This difficulty is compounded in environments where potentially hostile actors may be attempting to jam communications. This necessitates a level of automation on-board capable of maintaining flight while denied navigation information.\
		\
		There are many different types of approaches for this problem, but most involve some form of identifying targets in real time and reacting as they become visible to the aircraft. This has proven successful on a flying robot traveling at high speeds \\cite\{barry2015pushbroom\}. This system successfully combined trajectory libraries and a state machine to avoid obstacles using very little computational power even at very high speeds \\cite\{barry2018high\}. Another solution to obstacle avoidance on flying robots was the creation of NanoMap \\cite\{2018nanomap\}. This allows for 3D data to be processed at a much faster rate allowing for higher speeds of the robot \\cite\{2018nanomap\}.\
		\
		\\subsubsection\{Computer Vision\}\
		\
		Currently there are many different ways that computers can make high-level decisions based on digital image information. There are many methods to acquire, process, and analyze data from the real world using a camera. While this is a very broad field, we intend to focus especially on the aspects around motion estimation and object recognition. Both will be working with a video stream taken from a camera. \
		\
		Motion estimation can be accomplished using direct methods which compare whole fields of pixels to each other over successive frames, compared to indirect methods which look for specific features. The information resulting from motion estimation streams can be used to both compensate for motion while analyzing other aspects of an image, and update a state machine.\
		\
		Object recognition in our project will be accomplishing two tasks. Identifying a marker or target which will require more involved object recognition calculations, and very simple techniques, such as edge detection, to identify obstacles that exist in the path of the robot.\
		\
		\\subsubsection\{OpenCV\}\
		\
		The Open Source Computer Vision Library (OpenCV) of programming functions is a cross-platform and free for use collection of functions primarily aimed at real-time computer vision\\cite\{opencv\}. Most well documented techniques to accomplish all of the computer vision goals of our project have already been created and refined in OpenCV. For this reason we will be leaning heavily on OpenCV functions.\
		\
		\\subsubsection\{Gazebo\}\
		\
		Gazebo is a robot simulator that allows for creation of a realistic environment which includes both obstacles and markers similar to those being used in the lab. It will then be used to rapidly test algorithms.\
		\
		\\subsubsection\{Robot Operating System\}\
		\
		The Robot Operating System (ROS) is a distributed message system that allows for various sensors and processors to work together to control a robot. It is open source and has been integrated already with OpenCV and Gazebo. There are many additional tools for detecting obstacles, mapping the environment, planning paths, and much more. It is also a robust messaging system that has been proven to be capable of real-time processes.\
		\
		\\subsubsection\{TurtleBot\}\
		\
		The TurtleBot is a robot kit with open-source design and software. A TurtleBot is a robot built to the specification for TurtleBot Compatible Platforms\\cite\{wise_foote_2011\}. In our case this is a Kobuki Base, an Acer Netbook running Ubuntu with ROS packages added, an X-Box Kinect, and some mounting plates. \
		\
		The resulting robot looks like a disk supporting some circular shelves with a small laptop and a camera on the shelves. The base disk is 35.4cm in diameter, the topmost shelf is 42cm from the ground. The robot has a maximum speed of 0.65m/s. \
		\
		\\subsubsection\{AscTec Pelican\}\
		\
		The Ascending Technologies Pelican is a 65.1cm by 65.1cm quad-rotor  designed for research purposes\\cite\{asctec\}. It includes a camera, a high level and low level processor set up for computer vision and SLAM research. It is also capable of interfacing easily with other controllers and can carry up to a kilogram of additional gear.}