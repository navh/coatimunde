\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage[margin=3cm,twoside]{geometry}
\usepackage[super]{nth}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{hyperref}
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{bibliographyDID03.bib}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{DID - 03 SOR (Hebb \& Stephan)}
\fancyhead[LO,RE]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

% Title Page
\title{Statement of Requirements}
\author{Officer Cadet 27714 Amos Navarre Hebb\\and\\Officer Cadet 27555 Kara Stephan}

\usepackage{graphicx}

\begin{document}
	
\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		
		\LARGE\textsc{Royal Military College of Canada}\normalsize
		
		\vspace{0.2cm}
		
		\textsc{Department of Electrical and Computer Engineering}
		
		\vspace{1.5cm}
		
		\includegraphics[width=0.3\textwidth]{rmcLogo.png}
		
		\vspace{1.5cm}
		
		\LARGE{DID-03 - Statement of Requirements}\normalsize
		
		\vfill
		
		\textbf{Presented by:}\\Amos Navarre \textsc{Hebb} \& Kara \textsc{Stephan}\\
		\vspace{0.8cm}
		\textbf{Presented to:}\\Dr. Sidney \textsc{Givigi}
		\vspace{0.8cm}
		
		\today
		
	\end{center}
\end{titlepage}

% \begin{abstract}
% \end{abstract}

\tableofcontents
\newpage

\section{Introduction}

	\subsection{Document Purpose}
	
	Using Computer Optics for Analyzing Trajectories in Mostly Unknown, Navigation Denied, Environments (\textsc{coatimunde}) is the goal of this project. The purpose of this document is to outline the requirements for \textsc{coatimunde}. That is, what the project is to accomplish once all of the requirements outlined in this document have been completed and to what standard they shall be considered done. The benefits of meeting these requirements and solving this problem will be outlined and some possible use cases stated. This document will then identify the constraints that these requirements impose on this project.
	
	\subsection{Background}
	
	Both in the consumer and professional sectors the use of autonomous aerial vehicles is growing quickly. Currently these vehicles rely on skilled pilots to accomplish a very limited set of tasks. Adding obstacle avoidance capabilities to these vehicles and simplifying the task of following targets could allow for these systems to be used in many more situations. This section will give a quick background on obstacle avoidance, unmanned aircraft systems, computer vision, and the platforms we intend to use in this project.
	
		\subsubsection{Obstacle Avoidance}
		
		
		Obstacle avoidance is the task of satisfying a control objective, in this case moving toward a visual target, while subject to non-intersection or non-collision position constraints. Those later constraints are, in this case, to be dynamically created while moving in a reactive manner, instead of being pre-computed.
		
		\subsubsection{Unmanned Aircraft Systems}
		
		Very generally any powered vehicle that uses aerodynamic forces to provide lift that does not carry a human operator can be considered an unmanned aerial vehicle. Currently most of these vehicles make up a single component of a larger unmanned aircraft system. 
		
		An Unmanned aircraft system (UAS), or Remotely piloted aircraft system (RPAS), is an aircraft without a human pilot on-board, instead controlled from an operator on the ground. 
		
		Such a system can have varying levels of autonomy, something as simple as a model aircraft could be considered a UAS without any automation capabilities. A UAS capable of detecting, recognizing, identifying, and tracking targets of interest in complex environments and integrate with the systems required to process and fuse the collected information into actionable intelligence while operating in a low-to-medium threat environment is the current goal of the RPAS project by the RCAF \cite{RPAS}. 
		
		Flying a UAS requires a secure link to the operator off-board. Maintaining this link, particularly while flying close to the ground where more opportunities for interference are introduced, and in environments where potentially hostile actors may be attempting to jam communications, necessitate a level of automation on-board capable of maintaining flight while denied navigation information.
		
		There are many different types of approaches for this problem, but most involve some form of identifying targets in real time and reacting as they become visible to the aircraft. This has proven successful on a flying robot traveling at high speeds \cite{barry2015pushbroom}. This system successfully combined trajectory libraries and a state machine to avoid obstacles using very little computational power even at very high speeds \cite{barry2018high}. Another solution to obstacle avoidance on flying robots was the creation of NanoMap \cite{2018nanomap}. This allows for 3D data to be processed at a much faster rate allowing for higher speeds of the robot \cite{2018nanomap}.
		
		\subsubsection{Computer Vision}
		
		Currently there are many different ways that computers can make high-level decisions based on digital image information. There are many methods to acquire, process, and analyze data from the real world using a camera. While this is a very broad field, we intend to focus especially on the aspects around motion estimation and object recognition. Both will be working with a video stream taken from a camera. 
		
		Motion estimation can be accomplished using \textit{direct} methods which compare whole fields of pixels to each other over successive frames, or indirect methods which look for specific features. The information resulting from motion estimation streams can be used to both compensate for motion while analyzing other aspects of an image, and update a state machine.
		
		Object recognition in our project will be accomplishing two tasks. Identifying a marker or target which will require more involved object recognition calculations, and very simple techniques, such as edge detection, to identify obstacles that exist in the path of the robot.
		
		\subsubsection{OpenCV}
		
		The Open Source Computer Vision Library (OpenCV) of programming functions is a cross-platform and free for use collection of functions primarily aimed at real-time computer vision\cite{opencv}. Most well documented techniques to accomplish all of the computer vision goals of our project have already been created and refined in OpenCV. For this reason we will be leaning heavily on OpenCV functions.
		
		\subsubsection{Gazebo}
		
		Gazebo is a robot simulator that allows for creation of a realistic environment which includes both obstacles and makers similar to those being used in the lab. It will then be used to rapidly test algorithms.
		
		\subsubsection{Robot Operating System}
		
		The Robot Operating System (ROS) is a distributed message system that allows for various sensors and processors to work together to control a robot. It is open source and has been integrated already with OpenCV and Gazebo. There are many additional tools for detecting obstacles, mapping the environment, planning paths, and much more. It is also a robust messaging system that has been proven to be capable of real-time processes.
		
		\subsubsection{TurtleBot}
		
		The TurtleBot is a robot kit with open-source design and software. A TurtleBot is a robot built to the specification for TurtleBot Compatible Platforms\cite{wise_foote_2011}. In our case this is a Kobuki Base, an Acer Netbook running Ubuntu with ROS packages added, an X-Box Kinect, and some mounting plates. 
		
		The resulting robot looks like a disk supporting some circular shelves with a small laptop and a camera on the shelves. The base disk is 35.4cm in diameter, the topmost shelf is 42cm from the ground. The robot has a maximum speed of 0.65m/s. 
		
		\subsubsection{AscTec Pelican}
		
		The Ascending Technologies Pelican is a 65.1cm by 65.1cm quad-rotor  designed for research purposes\cite{asctec}. It includes a camera, a high level and low level processor set up for computer vision and SLAM research. It is also capable of interfacing easily with other controllers and can carry up to a kilogram of additional gear.
		
	\subsection{Definitions}
	
	Many terms used during this project have synonyms, and similarly there are many different ways to word many of the things in this project and often no conventions. For this purpose we will define terms which we intend to have more specific meanings than in normal English, or will clarify what we mean by the often vague descriptions used for many of the components of this project.
	
		\subsubsection{Flying Robot}
		
		A Flying Robot refers to any vehicle that is able to execute instructions while in the air under its own power.
		
		\subsubsection{Marker}
		
		A marker will be specifically a high contrast shape, such as one output by ArUco or a QR Code generator, that is optimized for identification by a camera.
		
	\subsection{Aim}
	
	The aim of this project is to build an air robot that is able to identify a target and move toward it, avoiding any obstacles that are in the way. 
	
	Tracking targets of interest in complex environments with a flying robot is the ultimate goal of this project. 
	
		\subsubsection{Targets of Interest}
		
		This target will be an object in the environment that has already been identified visually by an operator before navigation information is denied, or pre-programmed into the flying robot before operation.
		
		\subsubsection{Complex Environment}
		
		The proposed use case of this project would be an environment with obstacles that the flying robot could potentially collide with. Flying at sufficiently low altitude that trees or buildings could come between the flying robot and the target of interest is the core of the project.
		
		\subsubsection{Benefits}
		
		Having a flying robot capable of accomplishing these tasks while totally autonomous will allow for the use of flying robots in environments closer to the ground, and will assist pilots in complex environments. These general requirements can be used in many situations.
		
		\begin{itemize}
			\item \textbf{Surveillance:} The robot could follow an interesting object, especially in an urban environment, without colliding with obstacles.
			\item \textbf{Search and Rescue:} The robot could move toward visual way-points set by pilots while avoiding obstacles in complex environments assisting in search efforts.
			\item \textbf{Inspections:} The robot could inspect objects in hard to reach environments and complex environments like rooftops or bridges.
			\item \textbf{Disaster Relief:} The robot could check inside buildings that may have compromised structural integrity, rubble on ground, \textit{\&c}.
			\item \textbf{Agriculture:} The robot could inspect tree-fruit or crops that can not be observed from overhead or check assets in remote locations such as irrigation equipment.
		\end{itemize}
		
	\subsection{Scope}
	
	The project shall start by using a ground robot then processing to a flying robot. The project will use computer vision to find targets and for avoiding multiple obstacles. Both robots shall be highly autonomous, requiring only user input to commence. 

	The scope of the project is limited due to only being able to test indoors. Ideally the drone would be operating at a high speed and identifying an arbitrary target in an unpredictable environment. In our case though the testing must be completed within the confines of a relatively small robotics laboratory. Smaller obstacles will be used indoors simply due to space constraints. Lower speeds will be used as well, again due to the lack of space. 

\section{Requirement Definition Activities}

	\subsection{Information}
	
	To better define the requirements for this project the CAF RPAS project requirements were researched, to attempt to familiarize ourselves with the type of system outlined in the project \cite{RPAS}. Previously created obstacle avoidance systems implemented on similar UAVs were also researched to gain more background information for our knowledge. An article that was identified to be beneficial is about both target tracking and obstacle avoidance on a drone \cite{woods2015dynamic}. This outlined some specific problems that this project will likely face as well, and a possible solution to these problems. 
	The discussions with our project supervisor Dr. Givigi revealed that it was possible to build a monocular detection system that can identify targets and multiple obstacles at once. This then can be implemented on an air robot which can take proper action to avoid these obstacles and move towards the identified target. Those discussions inspired the project requirements detailed in the following section.

\section{Product Requirements}

	\subsection{Functional Requirements (FR)}
		
		\subsubsection{FR-01: Movement Toward Target}
		The air and land robots shall move toward a target under control.
		
		\subsubsection{FR-02: Identification of Target}
		The robot shall recognize optically a OpenCV's ArCuo shape and be able to give information on the targets location relative to the robot. 

		\subsubsection{FR-03: Identification of Obstacle}
		The robot shall optically recognize obstacles in its environment and identify where they are relative to itself. The obstacle will be defined as any solid opaque object that is at least the height of the camera on the robot at the time of detection.
		
		\subsubsection{FR-04: Avoiding Obstacle}
		The robot shall be able to make a deviation from its current movement pattern to avoid an obstacle in its path and then return to this pattern. The obstacle will be defined as any solid opaque object that is at least the height of the camera on the robot at the time of detection.
		
		\subsubsection{FR-05: Multiple Obstacles}
		The robot should be able identify multiple obstacles and avoid them accordingly. See FR-04. 
		
	\subsection{Performance Requirements (PR)}
	
		\subsubsection{PR-01: Target Identification}
		
		The robot will be able to identify and locate OpenCV's ArCuo shapes within a 15m radius. See FR-05.
		
		\subsubsection{PR-02: Interpretation of Visual Data}
		
		The flying robot should be able to process an image and plot obstacles at a rate of 10 Hz. See FR-07.
		
		\subsubsection{PR-03: Multiple Obstacles}
		
		The flying robot shall be able to navigate an environment and detect up to two obstacles simultaneously. See FR-08.
		
	\subsection{Interface Requirements (IR)}

		\subsubsection{IT-01: Gazebo Simulator}
		The Gazebo simulator shall be run on a laptop. Robots within the simulation will be interfaced through Robot Operating System.
	
		\subsubsection{IR-02: Turtlebot Communication through ROS}
		
		Communication to the Turtlebot will be done through Robot Operating System over USB.
		
		\subsubsection{IR-03: Air Robot Communication}
		
		The air robot will be communicating through Robot Operating System over wireless network.
		
		\subsubsection{IR-04: Off-board User Interface}
		
		A simple interface, perhaps command line, will allow a user to select a marker as a target to start the robot toward this target. The robot should receive this signal over a wireless transmission.
		
	\subsection{Simulation Requirements (SimR)}
	
		\subsubsection{SimR-01: Empty Lab TurtleBot in Gazebo}
		
		A Gazebo simulation environment which roughly approximates a lab environment with markers placed around will be created. A model of the TurtleBot will be able to navigate toward markers in this environment.
		
		\subsubsection{SimR-02: Lab with Obstacles TurtleBot in Gazebo}
		
		Stationary obstacles will be added to the Lab simulation environment and a TurtleBot shall navigate toward markers in the lab while avoiding these obstacles.
		
		\subsubsection{SimR-03: Lab with Obstacles Flying Robot}
		
		An environment with obstacles will be created and a Flying Robot shall navigate toward markers in the simulation while avoiding obstacles.
		
	
	\subsection{Implementation Requirements (ImpR)}
	
		\subsubsection{ImpR-01: Computer Vision}
		
		OpenCV programs shall be created that can use a single video stream and identify both markers and obstacles.
	
		\subsubsection{ImpR-02: Gazebo Simulation}
		
		Prior to testing on the Turtlebot, the program shall be implemented on the gazebo simulation. This simulation environment will be able to create a realistic video stream that can be used for testing.
		
		\subsubsection{ImpR-03: Robot Operating System}
		
		Using the robot in the simulation environment the appropriate components, tools, and libraries to interpret an OpenCV stream, make decisions based on the environment, and execute instructions will be developed.
		
		\subsubsection{ImpR-04: Turtlebot in Simulation}
		
		The simplest obstacle avoidance algorithm must be implemented on a Turtlebot using the Robot Operating System.
		
		\subsubsection{ImpR-05: Turtlebot in Lab}
		
		The Turtlebot will operate in the real world, identifying targets and avoiding obstacles. 
		
		\subsubsection{ImpR-06: Flying Robot in Simulation}
		
		The obstacle avoidance algorithm used for a flying robot will be implemented in a simulation avoiding obstacles and identifying targets.
		
		\subsubsection{ImpR-07: Flying Robot in Lab}
		
		The Flying Robot will operate in the lab, identifying targets and avoiding obstacles.
		
	\subsection{Schedule Restrictions (SchR)}
	
		\subsubsection{SchR-01: Simulation}
		
		The first TurtleBot simulation shall be able to operate in a Gazebo simulation environment no later than November \nth{5}. It shall be able to identify an ArUco shape as a marker, move toward the marker, and avoid a stationary obstacle placed into the simulation environment.
		
		\subsubsection{SchR-02: TurtleBot Prototype}
		
		The first functional prototype shall be a TurtleBot robot capable of positively identifying a marker, moving toward the marker, and avoiding an obstacle placed into its environment no later than December \nth{18}.
		
		\subsubsection{SchR-03: Flying Prototype}
		
		The first functional flying prototype shall be capable of identifying a marker, moving toward the marker, and avoiding an obstacle placed into its environment no later than February  \nth{18}.

\section{Risk Assessment}

	\subsection{Identifying Markers}
	
	The markers we intend to use, at least initially, are intended for use in Augmented Reality purposes. Tests of these shapes show that they are capable of being identified at many angles, but the reliability with which they can be identified decreases quickly as the viewing angle changes. 
	
	There is potential that markers which are reliably identified at low speeds and in simulation may not be detectable on a flying vehicle moving faster.
	
		\subsubsection{Likelihood}
 
		Both the ground and flying robots will be traveling at lower speeds so therefore the likelihood of this risk is low.
		
		\subsubsection{Impact}
		
		High impact, the primary task that our robot must accomplish is moving toward a visual target. If the robot is not able to reliably identify a marker then there will be no way to verify its capability.
		
		\subsubsection{Process Solution}
		
		Alternatives to the ArUco markers can be tested, or if these prove truly unreliable then coloured areas or illuminated targets could be incorporated. These are less desirable as ideally the robot would be able to fly toward an arbitrary target.
		
	\subsection{Computer Hardware Limitations}
	
	Image processing, especially quickly and with multiple goals, is computationally expensive. While this should not be an issue on a ground vehicle moving at slower speeds, it may become more of an issue on a flying robot if it operates at higher speeds and is incapable of carrying as much on-board computational capability.
	
		\subsubsection{Likelihood}

		High likelihood due to the amount of computational power required to do image processing. 
		
		\subsubsection{Impact}
		
		Medium impact, presumably we would still be able to prove our systems on the ground robot which is capable of carrying as much processing power as needed. It may also only be a limitation at higher speeds or may impose limits on what the air robot is capable of identifying and tracking. 
		
		\subsubsection{Process Solution}
		
		Testing many algorithms and working to streamline the algorithm used, especially for the flying robot, as well as selecting hardware that is complimentary to the kind of loads created by running such an algorithm can mitigate these risks. 
	
	\subsection{Flight Control System Inaccuracy}
	
	Many drones are not able to execute arbitrary movements with high accuracy. Verifying that the drone is actually executing instructions may be difficult. 
	
		\subsubsection{Likelihood}
		
		Medium likelihood, depending on how well developed the libraries for our particular drone these issues may have all been sufficiently worked out for the purposes of this project.
		
		\subsubsection{Impact}
		
		Low impact, we could still observe the flying robot making decisions even if it is unable to execute the instructions given properly.
		
		\subsubsection{Process Solution}
		
		Using smaller number of obstacles and moving at a slow enough speed should ensure that the robot never needs to execute extreme moves. These more extreme moves are where inaccuracies in control models will exposes themselves the most. 

\section{Conclusion}

The project requirement have been listed for the air robot obstacle avoidance system. Background and requirement defining activities have been given to lay the groundwork and a basic understand of the current research in this domain. This document will be used for guiding the project and aiding in the creation of the preliminary design. 


\printbibliography

%\end{multicols}
\end{document}          
